{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "nmfdf = pd.read_csv('preprocess_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>URL</th>\n",
       "      <th>transcript</th>\n",
       "      <th>Comedian Name</th>\n",
       "      <th>word_count</th>\n",
       "      <th>Unique ID</th>\n",
       "      <th>neg_polarity</th>\n",
       "      <th>neu_polarity</th>\n",
       "      <th>pos_polarity</th>\n",
       "      <th>compound</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>tokens_lst</th>\n",
       "      <th>select_tokens</th>\n",
       "      <th>all_tokens</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://scrapsfromtheloft.com/comedy/pete-holm...</td>\n",
       "      <td>\\n \\n audience cheering and applauding \\n hell...</td>\n",
       "      <td>Pete Holmes</td>\n",
       "      <td>10068</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.256</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.504545</td>\n",
       "      <td>['audience', 'cheering', 'applauding', 'hello'...</td>\n",
       "      <td>['audience', 'cheering', 'applauding', 'hello'...</td>\n",
       "      <td>[('audience', 'NN'), ('cheering', 'VBG'), ('ap...</td>\n",
       "      <td>553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://scrapsfromtheloft.com/comedy/jeff-dunh...</td>\n",
       "      <td>\\n \\n im funnier than he is but they told me t...</td>\n",
       "      <td>Jeff Dunham</td>\n",
       "      <td>4943</td>\n",
       "      <td>1</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.264</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.535705</td>\n",
       "      <td>['im', 'funnier', 'told', 'introduce', 'heres'...</td>\n",
       "      <td>['im', 'funnier', 'told', 'introduce', 'heres'...</td>\n",
       "      <td>[('im', 'NN'), ('funnier', 'NN'), ('told', 'VB...</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                                URL  \\\n",
       "0           0  https://scrapsfromtheloft.com/comedy/pete-holm...   \n",
       "1           1  https://scrapsfromtheloft.com/comedy/jeff-dunh...   \n",
       "\n",
       "                                          transcript Comedian Name  \\\n",
       "0  \\n \\n audience cheering and applauding \\n hell...   Pete Holmes   \n",
       "1  \\n \\n im funnier than he is but they told me t...   Jeff Dunham   \n",
       "\n",
       "   word_count  Unique ID  neg_polarity  neu_polarity  pos_polarity  compound  \\\n",
       "0       10068          0         0.056         0.688         0.256       1.0   \n",
       "1        4943          1         0.062         0.673         0.264       1.0   \n",
       "\n",
       "   subjectivity                                         tokens_lst  \\\n",
       "0      0.504545  ['audience', 'cheering', 'applauding', 'hello'...   \n",
       "1      0.535705  ['im', 'funnier', 'told', 'introduce', 'heres'...   \n",
       "\n",
       "                                       select_tokens  \\\n",
       "0  ['audience', 'cheering', 'applauding', 'hello'...   \n",
       "1  ['im', 'funnier', 'told', 'introduce', 'heres'...   \n",
       "\n",
       "                                          all_tokens  diff  \n",
       "0  [('audience', 'NN'), ('cheering', 'VBG'), ('ap...   553  \n",
       "1  [('im', 'NN'), ('funnier', 'NN'), ('told', 'VB...   168  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmfdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      ['audience', 'cheering', 'applauding', 'hello'...\n",
       "1      ['im', 'funnier', 'told', 'introduce', 'heres'...\n",
       "2      ['netflix', 'standup', 'comedy', 'special', 't...\n",
       "3      ['kevin', 'bridges', 'overdue', 'catchup', 'co...\n",
       "4      ['get', 'knees', 'jacqueline', 'novak', 'trans...\n",
       "                             ...                        \n",
       "468    ['jammin', 'new', 'york', 'george', 'carlins',...\n",
       "469    ['australian', 'comedian', 'jim', 'jefferies',...\n",
       "470    ['hello', 'im', 'thomas', 'im', 'glad', 'meet'...\n",
       "471    ['complaints', 'grievances', 'hbo', 'standup',...\n",
       "472    ['full', 'transcript', 'bad', 'ya', 'final', '...\n",
       "Name: select_tokens, Length: 473, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmfdf[\"select_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'URL', 'transcript', 'Comedian Name', 'word_count',\n",
       "       'Unique ID', 'neg_polarity', 'neu_polarity', 'pos_polarity', 'compound',\n",
       "       'subjectivity', 'tokens_lst', 'select_tokens', 'all_tokens', 'diff'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmfdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, LangDetectException\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return None\n",
    "\n",
    "\n",
    "nmfdf['language'] = nmfdf['transcript'].apply(detect_language)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>URL</th>\n",
       "      <th>transcript</th>\n",
       "      <th>Comedian Name</th>\n",
       "      <th>word_count</th>\n",
       "      <th>Unique ID</th>\n",
       "      <th>neg_polarity</th>\n",
       "      <th>neu_polarity</th>\n",
       "      <th>pos_polarity</th>\n",
       "      <th>compound</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>tokens_lst</th>\n",
       "      <th>select_tokens</th>\n",
       "      <th>all_tokens</th>\n",
       "      <th>diff</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://scrapsfromtheloft.com/comedy/pete-holm...</td>\n",
       "      <td>\\n \\n audience cheering and applauding \\n hell...</td>\n",
       "      <td>Pete Holmes</td>\n",
       "      <td>10068</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.256</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.504545</td>\n",
       "      <td>['audience', 'cheering', 'applauding', 'hello'...</td>\n",
       "      <td>['audience', 'cheering', 'applauding', 'hello'...</td>\n",
       "      <td>[('audience', 'NN'), ('cheering', 'VBG'), ('ap...</td>\n",
       "      <td>553</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://scrapsfromtheloft.com/comedy/jeff-dunh...</td>\n",
       "      <td>\\n \\n im funnier than he is but they told me t...</td>\n",
       "      <td>Jeff Dunham</td>\n",
       "      <td>4943</td>\n",
       "      <td>1</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.264</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.535705</td>\n",
       "      <td>['im', 'funnier', 'told', 'introduce', 'heres'...</td>\n",
       "      <td>['im', 'funnier', 'told', 'introduce', 'heres'...</td>\n",
       "      <td>[('im', 'NN'), ('funnier', 'NN'), ('told', 'VB...</td>\n",
       "      <td>168</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                                URL  \\\n",
       "0           0  https://scrapsfromtheloft.com/comedy/pete-holm...   \n",
       "1           1  https://scrapsfromtheloft.com/comedy/jeff-dunh...   \n",
       "\n",
       "                                          transcript Comedian Name  \\\n",
       "0  \\n \\n audience cheering and applauding \\n hell...   Pete Holmes   \n",
       "1  \\n \\n im funnier than he is but they told me t...   Jeff Dunham   \n",
       "\n",
       "   word_count  Unique ID  neg_polarity  neu_polarity  pos_polarity  compound  \\\n",
       "0       10068          0         0.056         0.688         0.256       1.0   \n",
       "1        4943          1         0.062         0.673         0.264       1.0   \n",
       "\n",
       "   subjectivity                                         tokens_lst  \\\n",
       "0      0.504545  ['audience', 'cheering', 'applauding', 'hello'...   \n",
       "1      0.535705  ['im', 'funnier', 'told', 'introduce', 'heres'...   \n",
       "\n",
       "                                       select_tokens  \\\n",
       "0  ['audience', 'cheering', 'applauding', 'hello'...   \n",
       "1  ['im', 'funnier', 'told', 'introduce', 'heres'...   \n",
       "\n",
       "                                          all_tokens  diff language  \n",
       "0  [('audience', 'NN'), ('cheering', 'VBG'), ('ap...   553       en  \n",
       "1  [('im', 'NN'), ('funnier', 'NN'), ('told', 'VB...   168       en  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmfdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_english_indices = nmfdf[nmfdf['language'] != 'en'].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[338, 339, 340, 382, 396, 407, 459, 460, 461]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_english_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>URL</th>\n",
       "      <th>transcript</th>\n",
       "      <th>Comedian Name</th>\n",
       "      <th>word_count</th>\n",
       "      <th>Unique ID</th>\n",
       "      <th>neg_polarity</th>\n",
       "      <th>neu_polarity</th>\n",
       "      <th>pos_polarity</th>\n",
       "      <th>compound</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>tokens_lst</th>\n",
       "      <th>select_tokens</th>\n",
       "      <th>all_tokens</th>\n",
       "      <th>diff</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>338</td>\n",
       "      <td>https://scrapsfromtheloft.com/comedy/dave-chap...</td>\n",
       "      <td>\\n \\n original english transcript  here \\n kil...</td>\n",
       "      <td>Dave Chappelle</td>\n",
       "      <td>6661</td>\n",
       "      <td>338</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.9984</td>\n",
       "      <td>0.567272</td>\n",
       "      <td>['original', 'english', 'transcript', 'killing...</td>\n",
       "      <td>['original', 'english', 'transcript', 'killing...</td>\n",
       "      <td>[('original', 'JJ'), ('english', 'JJ'), ('tran...</td>\n",
       "      <td>1135</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>339</td>\n",
       "      <td>https://scrapsfromtheloft.com/movies/dave-chap...</td>\n",
       "      <td>\\n \\n original english transcript  here \\n dav...</td>\n",
       "      <td>Dave Chappelle</td>\n",
       "      <td>5379</td>\n",
       "      <td>339</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.9992</td>\n",
       "      <td>0.635714</td>\n",
       "      <td>['original', 'english', 'transcript', 'dave', ...</td>\n",
       "      <td>['original', 'english', 'transcript', 'dave', ...</td>\n",
       "      <td>[('original', 'JJ'), ('english', 'JJ'), ('tran...</td>\n",
       "      <td>802</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>340</td>\n",
       "      <td>https://scrapsfromtheloft.com/comedy/dave-chap...</td>\n",
       "      <td>\\n \\n ho iniziato a 14 anni è stato allora che...</td>\n",
       "      <td>Dave Chappelle</td>\n",
       "      <td>2918</td>\n",
       "      <td>340</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.9547</td>\n",
       "      <td>0.487079</td>\n",
       "      <td>['ho', 'iniziato', '14', 'anni', 'è', 'stato',...</td>\n",
       "      <td>['ho', 'iniziato', 'anni', 'è', 'stato', 'allo...</td>\n",
       "      <td>[('ho', 'NN'), ('iniziato', 'NN'), ('14', 'CD'...</td>\n",
       "      <td>170</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>382</td>\n",
       "      <td>https://scrapsfromtheloft.com/comedy/jim-jeffe...</td>\n",
       "      <td>\\n \\n il comico australiano jim jefferies ridi...</td>\n",
       "      <td>Jim Jefferies</td>\n",
       "      <td>2010</td>\n",
       "      <td>382</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.9105</td>\n",
       "      <td>0.485000</td>\n",
       "      <td>['il', 'comico', 'australiano', 'jim', 'jeffer...</td>\n",
       "      <td>['il', 'comico', 'australiano', 'jim', 'jeffer...</td>\n",
       "      <td>[('il', 'NN'), ('comico', 'NN'), ('australiano...</td>\n",
       "      <td>182</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>396</td>\n",
       "      <td>https://scrapsfromtheloft.com/comedy/bill-burr...</td>\n",
       "      <td>\\n \\n va bene grazie grazie mille va bene gesù...</td>\n",
       "      <td>Bill Burr</td>\n",
       "      <td>7694</td>\n",
       "      <td>396</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.9416</td>\n",
       "      <td>0.544460</td>\n",
       "      <td>['va', 'bene', 'grazie', 'grazie', 'mille', 'v...</td>\n",
       "      <td>['va', 'bene', 'grazie', 'grazie', 'mille', 'v...</td>\n",
       "      <td>[('va', 'JJ'), ('bene', 'NN'), ('grazie', 'NN'...</td>\n",
       "      <td>494</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>407</td>\n",
       "      <td>https://scrapsfromtheloft.com/comedy/doug-stan...</td>\n",
       "      <td>\\n \\n new york  è sconcertante nel è una città...</td>\n",
       "      <td>Doug Stanhope</td>\n",
       "      <td>8026</td>\n",
       "      <td>407</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.6159</td>\n",
       "      <td>0.481450</td>\n",
       "      <td>['new', 'york', 'è', 'sconcertante', 'nel', 'è...</td>\n",
       "      <td>['new', 'york', 'è', 'sconcertante', 'nel', 'è...</td>\n",
       "      <td>[('new', 'JJ'), ('york', 'NN'), ('è', 'NN'), (...</td>\n",
       "      <td>623</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>459</td>\n",
       "      <td>https://scrapsfromtheloft.com/comedy/george-ca...</td>\n",
       "      <td>\\n \\n ciao grazie grazie grazie grazie molte g...</td>\n",
       "      <td>George Carlin</td>\n",
       "      <td>7906</td>\n",
       "      <td>459</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.9792</td>\n",
       "      <td>0.505548</td>\n",
       "      <td>['ciao', 'grazie', 'grazie', 'grazie', 'grazie...</td>\n",
       "      <td>['ciao', 'grazie', 'grazie', 'grazie', 'grazie...</td>\n",
       "      <td>[('ciao', 'NN'), ('grazie', 'NN'), ('grazie', ...</td>\n",
       "      <td>659</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>460</td>\n",
       "      <td>https://scrapsfromtheloft.com/comedy/george-ca...</td>\n",
       "      <td>\\n \\n siete gentili grazie grazie mille lo app...</td>\n",
       "      <td>George Carlin</td>\n",
       "      <td>9013</td>\n",
       "      <td>460</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.9927</td>\n",
       "      <td>0.431763</td>\n",
       "      <td>['siete', 'gentili', 'grazie', 'grazie', 'mill...</td>\n",
       "      <td>['siete', 'gentili', 'grazie', 'grazie', 'mill...</td>\n",
       "      <td>[('siete', 'JJ'), ('gentili', 'NN'), ('grazie'...</td>\n",
       "      <td>654</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>461</td>\n",
       "      <td>https://scrapsfromtheloft.com/comedy/george-ca...</td>\n",
       "      <td>\\n \\n grazie grazie grazie mi piacerebbe inizi...</td>\n",
       "      <td>George Carlin</td>\n",
       "      <td>9570</td>\n",
       "      <td>461</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.9888</td>\n",
       "      <td>0.432653</td>\n",
       "      <td>['grazie', 'grazie', 'grazie', 'mi', 'piacereb...</td>\n",
       "      <td>['grazie', 'grazie', 'grazie', 'mi', 'piacereb...</td>\n",
       "      <td>[('grazie', 'NN'), ('grazie', 'NN'), ('grazie'...</td>\n",
       "      <td>802</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                                URL  \\\n",
       "338         338  https://scrapsfromtheloft.com/comedy/dave-chap...   \n",
       "339         339  https://scrapsfromtheloft.com/movies/dave-chap...   \n",
       "340         340  https://scrapsfromtheloft.com/comedy/dave-chap...   \n",
       "382         382  https://scrapsfromtheloft.com/comedy/jim-jeffe...   \n",
       "396         396  https://scrapsfromtheloft.com/comedy/bill-burr...   \n",
       "407         407  https://scrapsfromtheloft.com/comedy/doug-stan...   \n",
       "459         459  https://scrapsfromtheloft.com/comedy/george-ca...   \n",
       "460         460  https://scrapsfromtheloft.com/comedy/george-ca...   \n",
       "461         461  https://scrapsfromtheloft.com/comedy/george-ca...   \n",
       "\n",
       "                                            transcript   Comedian Name  \\\n",
       "338  \\n \\n original english transcript  here \\n kil...  Dave Chappelle   \n",
       "339  \\n \\n original english transcript  here \\n dav...  Dave Chappelle   \n",
       "340  \\n \\n ho iniziato a 14 anni è stato allora che...  Dave Chappelle   \n",
       "382  \\n \\n il comico australiano jim jefferies ridi...   Jim Jefferies   \n",
       "396  \\n \\n va bene grazie grazie mille va bene gesù...       Bill Burr   \n",
       "407  \\n \\n new york  è sconcertante nel è una città...   Doug Stanhope   \n",
       "459  \\n \\n ciao grazie grazie grazie grazie molte g...   George Carlin   \n",
       "460  \\n \\n siete gentili grazie grazie mille lo app...   George Carlin   \n",
       "461  \\n \\n grazie grazie grazie mi piacerebbe inizi...   George Carlin   \n",
       "\n",
       "     word_count  Unique ID  neg_polarity  neu_polarity  pos_polarity  \\\n",
       "338        6661        338         0.045         0.940         0.015   \n",
       "339        5379        339         0.054         0.923         0.023   \n",
       "340        2918        340         0.011         0.971         0.018   \n",
       "382        2010        382         0.020         0.965         0.015   \n",
       "396        7694        396         0.011         0.976         0.013   \n",
       "407        8026        407         0.012         0.974         0.013   \n",
       "459        7906        459         0.007         0.982         0.011   \n",
       "460        9013        460         0.006         0.981         0.013   \n",
       "461        9570        461         0.009         0.978         0.013   \n",
       "\n",
       "     compound  subjectivity  \\\n",
       "338   -0.9984      0.567272   \n",
       "339   -0.9992      0.635714   \n",
       "340    0.9547      0.487079   \n",
       "382   -0.9105      0.485000   \n",
       "396    0.9416      0.544460   \n",
       "407   -0.6159      0.481450   \n",
       "459    0.9792      0.505548   \n",
       "460    0.9927      0.431763   \n",
       "461    0.9888      0.432653   \n",
       "\n",
       "                                            tokens_lst  \\\n",
       "338  ['original', 'english', 'transcript', 'killing...   \n",
       "339  ['original', 'english', 'transcript', 'dave', ...   \n",
       "340  ['ho', 'iniziato', '14', 'anni', 'è', 'stato',...   \n",
       "382  ['il', 'comico', 'australiano', 'jim', 'jeffer...   \n",
       "396  ['va', 'bene', 'grazie', 'grazie', 'mille', 'v...   \n",
       "407  ['new', 'york', 'è', 'sconcertante', 'nel', 'è...   \n",
       "459  ['ciao', 'grazie', 'grazie', 'grazie', 'grazie...   \n",
       "460  ['siete', 'gentili', 'grazie', 'grazie', 'mill...   \n",
       "461  ['grazie', 'grazie', 'grazie', 'mi', 'piacereb...   \n",
       "\n",
       "                                         select_tokens  \\\n",
       "338  ['original', 'english', 'transcript', 'killing...   \n",
       "339  ['original', 'english', 'transcript', 'dave', ...   \n",
       "340  ['ho', 'iniziato', 'anni', 'è', 'stato', 'allo...   \n",
       "382  ['il', 'comico', 'australiano', 'jim', 'jeffer...   \n",
       "396  ['va', 'bene', 'grazie', 'grazie', 'mille', 'v...   \n",
       "407  ['new', 'york', 'è', 'sconcertante', 'nel', 'è...   \n",
       "459  ['ciao', 'grazie', 'grazie', 'grazie', 'grazie...   \n",
       "460  ['siete', 'gentili', 'grazie', 'grazie', 'mill...   \n",
       "461  ['grazie', 'grazie', 'grazie', 'mi', 'piacereb...   \n",
       "\n",
       "                                            all_tokens  diff language  \n",
       "338  [('original', 'JJ'), ('english', 'JJ'), ('tran...  1135       es  \n",
       "339  [('original', 'JJ'), ('english', 'JJ'), ('tran...   802       es  \n",
       "340  [('ho', 'NN'), ('iniziato', 'NN'), ('14', 'CD'...   170       it  \n",
       "382  [('il', 'NN'), ('comico', 'NN'), ('australiano...   182       it  \n",
       "396  [('va', 'JJ'), ('bene', 'NN'), ('grazie', 'NN'...   494       it  \n",
       "407  [('new', 'JJ'), ('york', 'NN'), ('è', 'NN'), (...   623       it  \n",
       "459  [('ciao', 'NN'), ('grazie', 'NN'), ('grazie', ...   659       it  \n",
       "460  [('siete', 'JJ'), ('gentili', 'NN'), ('grazie'...   654       it  \n",
       "461  [('grazie', 'NN'), ('grazie', 'NN'), ('grazie'...   802       it  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmfdf.iloc[non_english_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmfdf.drop(non_english_indices, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(464, 16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmfdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saurabhsmac/Humor-Analysis---NLP/Humor-Analysis---NLP/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/saurabhsmac/Humor-Analysis---NLP/Humor-Analysis---NLP/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [' ', \"'\", 'a', 'b', 'c', 'd', 'f', 'h', 'i', 'j', 'k', 'm', 'o', 'p', 'q', 's', 't', 'u', 'v', 'w', 'y', 'z'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: life, dog, wife, eat, jesus, home, car, whole, head, work\n",
      "Topic 1: mum, bloke, wee, london, lovely, accent, fat, shite, brilliant, glasgow\n",
      "Topic 2: president, america, donald, country, obama, money, police, women, american, folks\n",
      "Topic 3: women, men, woman, girl, sex, gay, baby, life, bad, pregnant\n",
      "Topic 4: mom, dad, son, kid, life, parents, friends, baby, school, bro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saurabhsmac/Humor-Analysis---NLP/Humor-Analysis---NLP/.venv/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1770: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from ast import literal_eval\n",
    "nmfdf['select_tokens_nmf'] = nmfdf['select_tokens'].apply(literal_eval)\n",
    "\n",
    "custom_stopwords = [\n",
    "    \"know\", \"im\", \"dont\", \"thats\", \"get\", \"right\", \"go\", \"got\", \"youre\", \"people\",\n",
    "    \"laughing\", \"audience\", \"know\", \"im\", \"right\", \"dont\", \"applauding\", \"cheering\", \"go\", \"crowd\",\n",
    "    \"laughter\", \"im\", \"go\", \"know\", \"applause\", \"dont\", \"got\", \"yeah\", \"right\", \"people\",\n",
    "    \"g\", \"shit\", \"n\", \"got\", \"fuck\", \"man\", \"aint\", \"get\", \"dont\", \"motherfucker\",\n",
    "    \"laughs\", \"audience\", \"youre\", \"im\", \"go\", \"know\", \"dont\", \"goes\", \"get\", \"crowd\",\n",
    "    \"gon\", \"guys\", \"hes\", \"think\", \"guy\", \"want\", \"okay\", \"cause\", \"theyre\", \"oh\",\n",
    "    \"fucking\", \"gon\", \"shes\", \"guy\", \"going\", \"hes\", \"oh\", \"went\", \"dick\", \"back\",\n",
    "    \"said\", \"went\", \"ive\", \"didnt\", \"thought\", \"well\", \"back\", \"think\", \"say\", \"tell\",\n",
    "    \"gon\", \"white\", \"black\", \"see\", \"come\", \"say\", \"cause\", \"ass\", \"said\", \"cant\",\n",
    "    \"going\", \"think\", \"well\", \"theres\", \"say\", \"really\", \"want\", \"good\", \"theyre\", \"time\",\n",
    "    \"thing\", \"little\", \"look\", \"never\", \"ill\", \"way\", \"even\", \"mean\", \"bit\", \"things\", \"look\", \n",
    "    \"hey\", \"little\", \"let\", \"god\", \"ta\", \"take\", \"put\", \"bitch\", \"kids\", \"uh\", \"um\", \"hey\", \"voice\", \n",
    "    \"fcking\", \"fck\", \"sht\", \"chuckles\", \"something\", \"ever\", \"make\", \"lot\", \"always\", \"thank\", \"much\", \n",
    "    \"yall\", \"give\", \"damn\", \"goddamn\", \"pussy\", \"everybody\", \"feel\", \"girls\", \n",
    "    \"big\", \"youve\", \"yes\", \"show\", \"world\", \"quite\", \"years\", \"sort\", \"dude\", \"day\", \"whats\", \"first\",\n",
    "    \"room\", \"house\", \"night\", \"hell\", \"wan\", \"talking\", \"real\", \"somebody\", \"need\", \"talk\", \"new\", \"maybe\", \n",
    "    \"doesn't\", \"great\", \"old\", \"joke\", \"id\", \"isn't\", \"mate\", \"used\", \"actually\", \"cos\", \"done\", \"last\",\n",
    "    \"doesnt\", \"anything\", \"saying\", \"still\", \"remember\", \"someone\", \"hear\", \"lets\", \"crazy\", \"chris\", \n",
    "    \"nobody\", \"mama\", \"fuckin\", \"em\", \"face\", \"goin\", \"anyway\", \"happened\",\n",
    "    \"stuff\", \"weird\", \"getting\", \"trying\", \"ngga\", \"stop\", \"isnt\", \"ok\", \"love\", \"weve\",\n",
    "    \"kind\", \"point\", \"person\", \"away\", \"looking\", \"cheers\", \"kevin\", \"walk\", \"looking name\", \n",
    "    \"motherfuckers\", \"btch\", \"nggas\", \"e\", \"r\", \"motherfucking\", \"yo\",\n",
    "    \"trump\", \"ha\", \"everyone\",\"cunt\",\"heres\",\"er\",\"hello\",\"lee\",\"ricky\",\"dave\",\"bill\",\"round\",\"cock\",\"name\",\"word\",\"words\"\n",
    "    ,\"l\",\"theyve\",\"fucked\",\"imitates\",\"michael\",\"porn\"\n",
    "\n",
    "]\n",
    "def dummy_tokenizer(tokens):\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=dummy_tokenizer, preprocessor=lambda x: x, lowercase=False, stop_words=custom_stopwords)\n",
    "X = tfidf_vectorizer.fit_transform(nmfdf['select_tokens_nmf'])\n",
    "\n",
    "\n",
    "n_components = 5 \n",
    "nmf_model = NMF(n_components=n_components, random_state=42)\n",
    "W = nmf_model.fit_transform(X)\n",
    "H = nmf_model.components_\n",
    "\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(H):\n",
    "    top_features_ind = topic.argsort()[-10:][::-1]\n",
    "    top_features = [feature_names[i] for i in top_features_ind]\n",
    "    print(f\"Topic {topic_idx}: {', '.join(top_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
