{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "nmfdf = pd.read_csv('preprocess_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>URL</th>\n",
       "      <th>transcript</th>\n",
       "      <th>Comedian Name</th>\n",
       "      <th>word_count</th>\n",
       "      <th>Unique ID</th>\n",
       "      <th>neg_polarity</th>\n",
       "      <th>neu_polarity</th>\n",
       "      <th>pos_polarity</th>\n",
       "      <th>compound</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>tokens_lst</th>\n",
       "      <th>select_tokens</th>\n",
       "      <th>all_tokens</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://scrapsfromtheloft.com/comedy/pete-holm...</td>\n",
       "      <td>\\n \\n audience cheering and applauding \\n hell...</td>\n",
       "      <td>Pete Holmes</td>\n",
       "      <td>10068</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.256</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.504545</td>\n",
       "      <td>['audience', 'cheering', 'applauding', 'hello'...</td>\n",
       "      <td>['audience', 'cheering', 'applauding', 'hello'...</td>\n",
       "      <td>[('audience', 'NN'), ('cheering', 'VBG'), ('ap...</td>\n",
       "      <td>553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://scrapsfromtheloft.com/comedy/jeff-dunh...</td>\n",
       "      <td>\\n \\n im funnier than he is but they told me t...</td>\n",
       "      <td>Jeff Dunham</td>\n",
       "      <td>4943</td>\n",
       "      <td>1</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.264</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.535705</td>\n",
       "      <td>['im', 'funnier', 'told', 'introduce', 'heres'...</td>\n",
       "      <td>['im', 'funnier', 'told', 'introduce', 'heres'...</td>\n",
       "      <td>[('im', 'NN'), ('funnier', 'NN'), ('told', 'VB...</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                                URL  \\\n",
       "0           0  https://scrapsfromtheloft.com/comedy/pete-holm...   \n",
       "1           1  https://scrapsfromtheloft.com/comedy/jeff-dunh...   \n",
       "\n",
       "                                          transcript Comedian Name  \\\n",
       "0  \\n \\n audience cheering and applauding \\n hell...   Pete Holmes   \n",
       "1  \\n \\n im funnier than he is but they told me t...   Jeff Dunham   \n",
       "\n",
       "   word_count  Unique ID  neg_polarity  neu_polarity  pos_polarity  compound  \\\n",
       "0       10068          0         0.056         0.688         0.256       1.0   \n",
       "1        4943          1         0.062         0.673         0.264       1.0   \n",
       "\n",
       "   subjectivity                                         tokens_lst  \\\n",
       "0      0.504545  ['audience', 'cheering', 'applauding', 'hello'...   \n",
       "1      0.535705  ['im', 'funnier', 'told', 'introduce', 'heres'...   \n",
       "\n",
       "                                       select_tokens  \\\n",
       "0  ['audience', 'cheering', 'applauding', 'hello'...   \n",
       "1  ['im', 'funnier', 'told', 'introduce', 'heres'...   \n",
       "\n",
       "                                          all_tokens  diff  \n",
       "0  [('audience', 'NN'), ('cheering', 'VBG'), ('ap...   553  \n",
       "1  [('im', 'NN'), ('funnier', 'NN'), ('told', 'VB...   168  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmfdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      ['audience', 'cheering', 'applauding', 'hello'...\n",
       "1      ['im', 'funnier', 'told', 'introduce', 'heres'...\n",
       "2      ['netflix', 'standup', 'comedy', 'special', 't...\n",
       "3      ['kevin', 'bridges', 'overdue', 'catchup', 'co...\n",
       "4      ['get', 'knees', 'jacqueline', 'novak', 'trans...\n",
       "                             ...                        \n",
       "468    ['jammin', 'new', 'york', 'george', 'carlins',...\n",
       "469    ['australian', 'comedian', 'jim', 'jefferies',...\n",
       "470    ['hello', 'im', 'thomas', 'im', 'glad', 'meet'...\n",
       "471    ['complaints', 'grievances', 'hbo', 'standup',...\n",
       "472    ['full', 'transcript', 'bad', 'ya', 'final', '...\n",
       "Name: select_tokens, Length: 473, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmfdf[\"select_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'URL', 'transcript', 'Comedian Name', 'word_count',\n",
       "       'Unique ID', 'neg_polarity', 'neu_polarity', 'pos_polarity', 'compound',\n",
       "       'subjectivity', 'tokens_lst', 'select_tokens', 'all_tokens', 'diff'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmfdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, LangDetectException\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return None\n",
    "\n",
    "\n",
    "nmfdf['language'] = nmfdf['transcript'].apply(detect_language)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>URL</th>\n",
       "      <th>transcript</th>\n",
       "      <th>Comedian Name</th>\n",
       "      <th>word_count</th>\n",
       "      <th>Unique ID</th>\n",
       "      <th>neg_polarity</th>\n",
       "      <th>neu_polarity</th>\n",
       "      <th>pos_polarity</th>\n",
       "      <th>compound</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>tokens_lst</th>\n",
       "      <th>select_tokens</th>\n",
       "      <th>all_tokens</th>\n",
       "      <th>diff</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://scrapsfromtheloft.com/comedy/pete-holm...</td>\n",
       "      <td>\\n \\n audience cheering and applauding \\n hell...</td>\n",
       "      <td>Pete Holmes</td>\n",
       "      <td>10068</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.256</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.504545</td>\n",
       "      <td>['audience', 'cheering', 'applauding', 'hello'...</td>\n",
       "      <td>['audience', 'cheering', 'applauding', 'hello'...</td>\n",
       "      <td>[('audience', 'NN'), ('cheering', 'VBG'), ('ap...</td>\n",
       "      <td>553</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://scrapsfromtheloft.com/comedy/jeff-dunh...</td>\n",
       "      <td>\\n \\n im funnier than he is but they told me t...</td>\n",
       "      <td>Jeff Dunham</td>\n",
       "      <td>4943</td>\n",
       "      <td>1</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.264</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.535705</td>\n",
       "      <td>['im', 'funnier', 'told', 'introduce', 'heres'...</td>\n",
       "      <td>['im', 'funnier', 'told', 'introduce', 'heres'...</td>\n",
       "      <td>[('im', 'NN'), ('funnier', 'NN'), ('told', 'VB...</td>\n",
       "      <td>168</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                                URL  \\\n",
       "0           0  https://scrapsfromtheloft.com/comedy/pete-holm...   \n",
       "1           1  https://scrapsfromtheloft.com/comedy/jeff-dunh...   \n",
       "\n",
       "                                          transcript Comedian Name  \\\n",
       "0  \\n \\n audience cheering and applauding \\n hell...   Pete Holmes   \n",
       "1  \\n \\n im funnier than he is but they told me t...   Jeff Dunham   \n",
       "\n",
       "   word_count  Unique ID  neg_polarity  neu_polarity  pos_polarity  compound  \\\n",
       "0       10068          0         0.056         0.688         0.256       1.0   \n",
       "1        4943          1         0.062         0.673         0.264       1.0   \n",
       "\n",
       "   subjectivity                                         tokens_lst  \\\n",
       "0      0.504545  ['audience', 'cheering', 'applauding', 'hello'...   \n",
       "1      0.535705  ['im', 'funnier', 'told', 'introduce', 'heres'...   \n",
       "\n",
       "                                       select_tokens  \\\n",
       "0  ['audience', 'cheering', 'applauding', 'hello'...   \n",
       "1  ['im', 'funnier', 'told', 'introduce', 'heres'...   \n",
       "\n",
       "                                          all_tokens  diff language  \n",
       "0  [('audience', 'NN'), ('cheering', 'VBG'), ('ap...   553       en  \n",
       "1  [('im', 'NN'), ('funnier', 'NN'), ('told', 'VB...   168       en  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmfdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_english_indices = nmfdf[nmfdf['language'] != 'en'].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[338, 339, 340, 382, 396, 407, 459, 460, 461]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_english_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>URL</th>\n",
       "      <th>transcript</th>\n",
       "      <th>Comedian Name</th>\n",
       "      <th>word_count</th>\n",
       "      <th>Unique ID</th>\n",
       "      <th>neg_polarity</th>\n",
       "      <th>neu_polarity</th>\n",
       "      <th>pos_polarity</th>\n",
       "      <th>compound</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>tokens_lst</th>\n",
       "      <th>select_tokens</th>\n",
       "      <th>all_tokens</th>\n",
       "      <th>diff</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>338</td>\n",
       "      <td>https://scrapsfromtheloft.com/comedy/dave-chap...</td>\n",
       "      <td>\\n \\n original english transcript  here \\n kil...</td>\n",
       "      <td>Dave Chappelle</td>\n",
       "      <td>6661</td>\n",
       "      <td>338</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.9984</td>\n",
       "      <td>0.567272</td>\n",
       "      <td>['original', 'english', 'transcript', 'killing...</td>\n",
       "      <td>['original', 'english', 'transcript', 'killing...</td>\n",
       "      <td>[('original', 'JJ'), ('english', 'JJ'), ('tran...</td>\n",
       "      <td>1135</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>339</td>\n",
       "      <td>https://scrapsfromtheloft.com/movies/dave-chap...</td>\n",
       "      <td>\\n \\n original english transcript  here \\n dav...</td>\n",
       "      <td>Dave Chappelle</td>\n",
       "      <td>5379</td>\n",
       "      <td>339</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.9992</td>\n",
       "      <td>0.635714</td>\n",
       "      <td>['original', 'english', 'transcript', 'dave', ...</td>\n",
       "      <td>['original', 'english', 'transcript', 'dave', ...</td>\n",
       "      <td>[('original', 'JJ'), ('english', 'JJ'), ('tran...</td>\n",
       "      <td>802</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>340</td>\n",
       "      <td>https://scrapsfromtheloft.com/comedy/dave-chap...</td>\n",
       "      <td>\\n \\n ho iniziato a 14 anni è stato allora che...</td>\n",
       "      <td>Dave Chappelle</td>\n",
       "      <td>2918</td>\n",
       "      <td>340</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.9547</td>\n",
       "      <td>0.487079</td>\n",
       "      <td>['ho', 'iniziato', '14', 'anni', 'è', 'stato',...</td>\n",
       "      <td>['ho', 'iniziato', 'anni', 'è', 'stato', 'allo...</td>\n",
       "      <td>[('ho', 'NN'), ('iniziato', 'NN'), ('14', 'CD'...</td>\n",
       "      <td>170</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>382</td>\n",
       "      <td>https://scrapsfromtheloft.com/comedy/jim-jeffe...</td>\n",
       "      <td>\\n \\n il comico australiano jim jefferies ridi...</td>\n",
       "      <td>Jim Jefferies</td>\n",
       "      <td>2010</td>\n",
       "      <td>382</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.9105</td>\n",
       "      <td>0.485000</td>\n",
       "      <td>['il', 'comico', 'australiano', 'jim', 'jeffer...</td>\n",
       "      <td>['il', 'comico', 'australiano', 'jim', 'jeffer...</td>\n",
       "      <td>[('il', 'NN'), ('comico', 'NN'), ('australiano...</td>\n",
       "      <td>182</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>396</td>\n",
       "      <td>https://scrapsfromtheloft.com/comedy/bill-burr...</td>\n",
       "      <td>\\n \\n va bene grazie grazie mille va bene gesù...</td>\n",
       "      <td>Bill Burr</td>\n",
       "      <td>7694</td>\n",
       "      <td>396</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.9416</td>\n",
       "      <td>0.544460</td>\n",
       "      <td>['va', 'bene', 'grazie', 'grazie', 'mille', 'v...</td>\n",
       "      <td>['va', 'bene', 'grazie', 'grazie', 'mille', 'v...</td>\n",
       "      <td>[('va', 'JJ'), ('bene', 'NN'), ('grazie', 'NN'...</td>\n",
       "      <td>494</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>407</td>\n",
       "      <td>https://scrapsfromtheloft.com/comedy/doug-stan...</td>\n",
       "      <td>\\n \\n new york  è sconcertante nel è una città...</td>\n",
       "      <td>Doug Stanhope</td>\n",
       "      <td>8026</td>\n",
       "      <td>407</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.6159</td>\n",
       "      <td>0.481450</td>\n",
       "      <td>['new', 'york', 'è', 'sconcertante', 'nel', 'è...</td>\n",
       "      <td>['new', 'york', 'è', 'sconcertante', 'nel', 'è...</td>\n",
       "      <td>[('new', 'JJ'), ('york', 'NN'), ('è', 'NN'), (...</td>\n",
       "      <td>623</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>459</td>\n",
       "      <td>https://scrapsfromtheloft.com/comedy/george-ca...</td>\n",
       "      <td>\\n \\n ciao grazie grazie grazie grazie molte g...</td>\n",
       "      <td>George Carlin</td>\n",
       "      <td>7906</td>\n",
       "      <td>459</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.9792</td>\n",
       "      <td>0.505548</td>\n",
       "      <td>['ciao', 'grazie', 'grazie', 'grazie', 'grazie...</td>\n",
       "      <td>['ciao', 'grazie', 'grazie', 'grazie', 'grazie...</td>\n",
       "      <td>[('ciao', 'NN'), ('grazie', 'NN'), ('grazie', ...</td>\n",
       "      <td>659</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>460</td>\n",
       "      <td>https://scrapsfromtheloft.com/comedy/george-ca...</td>\n",
       "      <td>\\n \\n siete gentili grazie grazie mille lo app...</td>\n",
       "      <td>George Carlin</td>\n",
       "      <td>9013</td>\n",
       "      <td>460</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.9927</td>\n",
       "      <td>0.431763</td>\n",
       "      <td>['siete', 'gentili', 'grazie', 'grazie', 'mill...</td>\n",
       "      <td>['siete', 'gentili', 'grazie', 'grazie', 'mill...</td>\n",
       "      <td>[('siete', 'JJ'), ('gentili', 'NN'), ('grazie'...</td>\n",
       "      <td>654</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>461</td>\n",
       "      <td>https://scrapsfromtheloft.com/comedy/george-ca...</td>\n",
       "      <td>\\n \\n grazie grazie grazie mi piacerebbe inizi...</td>\n",
       "      <td>George Carlin</td>\n",
       "      <td>9570</td>\n",
       "      <td>461</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.9888</td>\n",
       "      <td>0.432653</td>\n",
       "      <td>['grazie', 'grazie', 'grazie', 'mi', 'piacereb...</td>\n",
       "      <td>['grazie', 'grazie', 'grazie', 'mi', 'piacereb...</td>\n",
       "      <td>[('grazie', 'NN'), ('grazie', 'NN'), ('grazie'...</td>\n",
       "      <td>802</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                                URL  \\\n",
       "338         338  https://scrapsfromtheloft.com/comedy/dave-chap...   \n",
       "339         339  https://scrapsfromtheloft.com/movies/dave-chap...   \n",
       "340         340  https://scrapsfromtheloft.com/comedy/dave-chap...   \n",
       "382         382  https://scrapsfromtheloft.com/comedy/jim-jeffe...   \n",
       "396         396  https://scrapsfromtheloft.com/comedy/bill-burr...   \n",
       "407         407  https://scrapsfromtheloft.com/comedy/doug-stan...   \n",
       "459         459  https://scrapsfromtheloft.com/comedy/george-ca...   \n",
       "460         460  https://scrapsfromtheloft.com/comedy/george-ca...   \n",
       "461         461  https://scrapsfromtheloft.com/comedy/george-ca...   \n",
       "\n",
       "                                            transcript   Comedian Name  \\\n",
       "338  \\n \\n original english transcript  here \\n kil...  Dave Chappelle   \n",
       "339  \\n \\n original english transcript  here \\n dav...  Dave Chappelle   \n",
       "340  \\n \\n ho iniziato a 14 anni è stato allora che...  Dave Chappelle   \n",
       "382  \\n \\n il comico australiano jim jefferies ridi...   Jim Jefferies   \n",
       "396  \\n \\n va bene grazie grazie mille va bene gesù...       Bill Burr   \n",
       "407  \\n \\n new york  è sconcertante nel è una città...   Doug Stanhope   \n",
       "459  \\n \\n ciao grazie grazie grazie grazie molte g...   George Carlin   \n",
       "460  \\n \\n siete gentili grazie grazie mille lo app...   George Carlin   \n",
       "461  \\n \\n grazie grazie grazie mi piacerebbe inizi...   George Carlin   \n",
       "\n",
       "     word_count  Unique ID  neg_polarity  neu_polarity  pos_polarity  \\\n",
       "338        6661        338         0.045         0.940         0.015   \n",
       "339        5379        339         0.054         0.923         0.023   \n",
       "340        2918        340         0.011         0.971         0.018   \n",
       "382        2010        382         0.020         0.965         0.015   \n",
       "396        7694        396         0.011         0.976         0.013   \n",
       "407        8026        407         0.012         0.974         0.013   \n",
       "459        7906        459         0.007         0.982         0.011   \n",
       "460        9013        460         0.006         0.981         0.013   \n",
       "461        9570        461         0.009         0.978         0.013   \n",
       "\n",
       "     compound  subjectivity  \\\n",
       "338   -0.9984      0.567272   \n",
       "339   -0.9992      0.635714   \n",
       "340    0.9547      0.487079   \n",
       "382   -0.9105      0.485000   \n",
       "396    0.9416      0.544460   \n",
       "407   -0.6159      0.481450   \n",
       "459    0.9792      0.505548   \n",
       "460    0.9927      0.431763   \n",
       "461    0.9888      0.432653   \n",
       "\n",
       "                                            tokens_lst  \\\n",
       "338  ['original', 'english', 'transcript', 'killing...   \n",
       "339  ['original', 'english', 'transcript', 'dave', ...   \n",
       "340  ['ho', 'iniziato', '14', 'anni', 'è', 'stato',...   \n",
       "382  ['il', 'comico', 'australiano', 'jim', 'jeffer...   \n",
       "396  ['va', 'bene', 'grazie', 'grazie', 'mille', 'v...   \n",
       "407  ['new', 'york', 'è', 'sconcertante', 'nel', 'è...   \n",
       "459  ['ciao', 'grazie', 'grazie', 'grazie', 'grazie...   \n",
       "460  ['siete', 'gentili', 'grazie', 'grazie', 'mill...   \n",
       "461  ['grazie', 'grazie', 'grazie', 'mi', 'piacereb...   \n",
       "\n",
       "                                         select_tokens  \\\n",
       "338  ['original', 'english', 'transcript', 'killing...   \n",
       "339  ['original', 'english', 'transcript', 'dave', ...   \n",
       "340  ['ho', 'iniziato', 'anni', 'è', 'stato', 'allo...   \n",
       "382  ['il', 'comico', 'australiano', 'jim', 'jeffer...   \n",
       "396  ['va', 'bene', 'grazie', 'grazie', 'mille', 'v...   \n",
       "407  ['new', 'york', 'è', 'sconcertante', 'nel', 'è...   \n",
       "459  ['ciao', 'grazie', 'grazie', 'grazie', 'grazie...   \n",
       "460  ['siete', 'gentili', 'grazie', 'grazie', 'mill...   \n",
       "461  ['grazie', 'grazie', 'grazie', 'mi', 'piacereb...   \n",
       "\n",
       "                                            all_tokens  diff language  \n",
       "338  [('original', 'JJ'), ('english', 'JJ'), ('tran...  1135       es  \n",
       "339  [('original', 'JJ'), ('english', 'JJ'), ('tran...   802       es  \n",
       "340  [('ho', 'NN'), ('iniziato', 'NN'), ('14', 'CD'...   170       it  \n",
       "382  [('il', 'NN'), ('comico', 'NN'), ('australiano...   182       it  \n",
       "396  [('va', 'JJ'), ('bene', 'NN'), ('grazie', 'NN'...   494       it  \n",
       "407  [('new', 'JJ'), ('york', 'NN'), ('è', 'NN'), (...   623       it  \n",
       "459  [('ciao', 'NN'), ('grazie', 'NN'), ('grazie', ...   659       it  \n",
       "460  [('siete', 'JJ'), ('gentili', 'NN'), ('grazie'...   654       it  \n",
       "461  [('grazie', 'NN'), ('grazie', 'NN'), ('grazie'...   802       it  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmfdf.iloc[non_english_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmfdf.drop(non_english_indices, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(464, 16)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmfdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saurabhsmac/Humor-Analysis---NLP/Humor-Analysis---NLP/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/saurabhsmac/Humor-Analysis---NLP/Humor-Analysis---NLP/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [' ', \"'\", 'a', 'b', 'c', 'd', 'f', 'h', 'i', 'j', 'k', 'm', 'o', 'p', 'q', 's', 't', 'u', 'v', 'w', 'y', 'z'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: life, dog, wife, eat, jesus, home, car, whole, head, work\n",
      "Topic 1: mum, bloke, wee, london, lovely, accent, fat, shite, brilliant, glasgow\n",
      "Topic 2: president, america, donald, country, obama, money, police, women, american, folks\n",
      "Topic 3: women, men, woman, girl, sex, gay, baby, life, bad, pregnant\n",
      "Topic 4: mom, dad, son, kid, life, parents, friends, baby, school, bro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saurabhsmac/Humor-Analysis---NLP/Humor-Analysis---NLP/.venv/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1770: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from ast import literal_eval\n",
    "nmfdf['select_tokens_nmf'] = nmfdf['select_tokens'].apply(literal_eval)\n",
    "\n",
    "custom_stopwords = [\n",
    "    \"know\", \"im\", \"dont\", \"thats\", \"get\", \"right\", \"go\", \"got\", \"youre\", \"people\",\n",
    "    \"laughing\", \"audience\", \"know\", \"im\", \"right\", \"dont\", \"applauding\", \"cheering\", \"go\", \"crowd\",\n",
    "    \"laughter\", \"im\", \"go\", \"know\", \"applause\", \"dont\", \"got\", \"yeah\", \"right\", \"people\",\n",
    "    \"g\", \"shit\", \"n\", \"got\", \"fuck\", \"man\", \"aint\", \"get\", \"dont\", \"motherfucker\",\n",
    "    \"laughs\", \"audience\", \"youre\", \"im\", \"go\", \"know\", \"dont\", \"goes\", \"get\", \"crowd\",\n",
    "    \"gon\", \"guys\", \"hes\", \"think\", \"guy\", \"want\", \"okay\", \"cause\", \"theyre\", \"oh\",\n",
    "    \"fucking\", \"gon\", \"shes\", \"guy\", \"going\", \"hes\", \"oh\", \"went\", \"dick\", \"back\",\n",
    "    \"said\", \"went\", \"ive\", \"didnt\", \"thought\", \"well\", \"back\", \"think\", \"say\", \"tell\",\n",
    "    \"gon\", \"white\", \"black\", \"see\", \"come\", \"say\", \"cause\", \"ass\", \"said\", \"cant\",\n",
    "    \"going\", \"think\", \"well\", \"theres\", \"say\", \"really\", \"want\", \"good\", \"theyre\", \"time\",\n",
    "    \"thing\", \"little\", \"look\", \"never\", \"ill\", \"way\", \"even\", \"mean\", \"bit\", \"things\", \"look\", \n",
    "    \"hey\", \"little\", \"let\", \"god\", \"ta\", \"take\", \"put\", \"bitch\", \"kids\", \"uh\", \"um\", \"hey\", \"voice\", \n",
    "    \"fcking\", \"fck\", \"sht\", \"chuckles\", \"something\", \"ever\", \"make\", \"lot\", \"always\", \"thank\", \"much\", \n",
    "    \"yall\", \"give\", \"damn\", \"goddamn\", \"pussy\", \"everybody\", \"feel\", \"girls\", \n",
    "    \"big\", \"youve\", \"yes\", \"show\", \"world\", \"quite\", \"years\", \"sort\", \"dude\", \"day\", \"whats\", \"first\",\n",
    "    \"room\", \"house\", \"night\", \"hell\", \"wan\", \"talking\", \"real\", \"somebody\", \"need\", \"talk\", \"new\", \"maybe\", \n",
    "    \"doesn't\", \"great\", \"old\", \"joke\", \"id\", \"isn't\", \"mate\", \"used\", \"actually\", \"cos\", \"done\", \"last\",\n",
    "    \"doesnt\", \"anything\", \"saying\", \"still\", \"remember\", \"someone\", \"hear\", \"lets\", \"crazy\", \"chris\", \n",
    "    \"nobody\", \"mama\", \"fuckin\", \"em\", \"face\", \"goin\", \"anyway\", \"happened\",\n",
    "    \"stuff\", \"weird\", \"getting\", \"trying\", \"ngga\", \"stop\", \"isnt\", \"ok\", \"love\", \"weve\",\n",
    "    \"kind\", \"point\", \"person\", \"away\", \"looking\", \"cheers\", \"kevin\", \"walk\", \"looking name\", \n",
    "    \"motherfuckers\", \"btch\", \"nggas\", \"e\", \"r\", \"motherfucking\", \"yo\",\n",
    "    \"trump\", \"ha\", \"everyone\",\"cunt\",\"heres\",\"er\",\"hello\",\"lee\",\"ricky\",\"dave\",\"bill\",\"round\",\"cock\",\"name\",\"word\",\"words\"\n",
    "    ,\"l\",\"theyve\",\"fucked\",\"imitates\",\"michael\",\"porn\"\n",
    "\n",
    "]\n",
    "def dummy_tokenizer(tokens):\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=dummy_tokenizer, preprocessor=lambda x: x, lowercase=False, stop_words=custom_stopwords)\n",
    "X = tfidf_vectorizer.fit_transform(nmfdf['select_tokens_nmf'])\n",
    "\n",
    "\n",
    "n_components = 5 \n",
    "nmf_model = NMF(n_components=n_components, random_state=42)\n",
    "W = nmf_model.fit_transform(X)\n",
    "H = nmf_model.components_\n",
    "\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(H):\n",
    "    top_features_ind = topic.argsort()[-10:][::-1]\n",
    "    top_features = [feature_names[i] for i in top_features_ind]\n",
    "    print(f\"Topic {topic_idx}: {', '.join(top_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 has dominant topic 4\n",
      "Document 1 has dominant topic 0\n",
      "Document 2 has dominant topic 3\n",
      "Document 3 has dominant topic 1\n",
      "Document 4 has dominant topic 3\n",
      "Document 5 has dominant topic 3\n",
      "Document 6 has dominant topic 1\n",
      "Document 7 has dominant topic 0\n",
      "Document 8 has dominant topic 1\n",
      "Document 9 has dominant topic 3\n",
      "Document 10 has dominant topic 4\n",
      "Document 11 has dominant topic 4\n",
      "Document 12 has dominant topic 1\n",
      "Document 13 has dominant topic 3\n",
      "Document 14 has dominant topic 4\n",
      "Document 15 has dominant topic 0\n",
      "Document 16 has dominant topic 4\n",
      "Document 17 has dominant topic 2\n",
      "Document 18 has dominant topic 4\n",
      "Document 19 has dominant topic 0\n",
      "Document 20 has dominant topic 3\n",
      "Document 21 has dominant topic 2\n",
      "Document 22 has dominant topic 2\n",
      "Document 23 has dominant topic 2\n",
      "Document 24 has dominant topic 2\n",
      "Document 25 has dominant topic 2\n",
      "Document 26 has dominant topic 0\n",
      "Document 27 has dominant topic 1\n",
      "Document 28 has dominant topic 0\n",
      "Document 29 has dominant topic 0\n",
      "Document 30 has dominant topic 2\n",
      "Document 31 has dominant topic 4\n",
      "Document 32 has dominant topic 2\n",
      "Document 33 has dominant topic 4\n",
      "Document 34 has dominant topic 0\n",
      "Document 35 has dominant topic 3\n",
      "Document 36 has dominant topic 4\n",
      "Document 37 has dominant topic 3\n",
      "Document 38 has dominant topic 3\n",
      "Document 39 has dominant topic 2\n",
      "Document 40 has dominant topic 0\n",
      "Document 41 has dominant topic 3\n",
      "Document 42 has dominant topic 0\n",
      "Document 43 has dominant topic 0\n",
      "Document 44 has dominant topic 3\n",
      "Document 45 has dominant topic 4\n",
      "Document 46 has dominant topic 1\n",
      "Document 47 has dominant topic 4\n",
      "Document 48 has dominant topic 0\n",
      "Document 49 has dominant topic 0\n",
      "Document 50 has dominant topic 1\n",
      "Document 51 has dominant topic 4\n",
      "Document 52 has dominant topic 0\n",
      "Document 53 has dominant topic 4\n",
      "Document 54 has dominant topic 4\n",
      "Document 55 has dominant topic 0\n",
      "Document 56 has dominant topic 4\n",
      "Document 57 has dominant topic 2\n",
      "Document 58 has dominant topic 0\n",
      "Document 59 has dominant topic 0\n",
      "Document 60 has dominant topic 0\n",
      "Document 61 has dominant topic 3\n",
      "Document 62 has dominant topic 2\n",
      "Document 63 has dominant topic 2\n",
      "Document 64 has dominant topic 3\n",
      "Document 65 has dominant topic 4\n",
      "Document 66 has dominant topic 0\n",
      "Document 67 has dominant topic 3\n",
      "Document 68 has dominant topic 3\n",
      "Document 69 has dominant topic 4\n",
      "Document 70 has dominant topic 3\n",
      "Document 71 has dominant topic 0\n",
      "Document 72 has dominant topic 0\n",
      "Document 73 has dominant topic 0\n",
      "Document 74 has dominant topic 3\n",
      "Document 75 has dominant topic 4\n",
      "Document 76 has dominant topic 4\n",
      "Document 77 has dominant topic 4\n",
      "Document 78 has dominant topic 3\n",
      "Document 79 has dominant topic 0\n",
      "Document 80 has dominant topic 1\n",
      "Document 81 has dominant topic 2\n",
      "Document 82 has dominant topic 2\n",
      "Document 83 has dominant topic 3\n",
      "Document 84 has dominant topic 2\n",
      "Document 85 has dominant topic 4\n",
      "Document 86 has dominant topic 4\n",
      "Document 87 has dominant topic 4\n",
      "Document 88 has dominant topic 4\n",
      "Document 89 has dominant topic 2\n",
      "Document 90 has dominant topic 3\n",
      "Document 91 has dominant topic 4\n",
      "Document 92 has dominant topic 0\n",
      "Document 93 has dominant topic 0\n",
      "Document 94 has dominant topic 3\n",
      "Document 95 has dominant topic 4\n",
      "Document 96 has dominant topic 4\n",
      "Document 97 has dominant topic 2\n",
      "Document 98 has dominant topic 0\n",
      "Document 99 has dominant topic 0\n",
      "Document 100 has dominant topic 1\n",
      "Document 101 has dominant topic 2\n",
      "Document 102 has dominant topic 0\n",
      "Document 103 has dominant topic 0\n",
      "Document 104 has dominant topic 0\n",
      "Document 105 has dominant topic 4\n",
      "Document 106 has dominant topic 0\n",
      "Document 107 has dominant topic 0\n",
      "Document 108 has dominant topic 4\n",
      "Document 109 has dominant topic 0\n",
      "Document 110 has dominant topic 1\n",
      "Document 111 has dominant topic 0\n",
      "Document 112 has dominant topic 2\n",
      "Document 113 has dominant topic 0\n",
      "Document 114 has dominant topic 2\n",
      "Document 115 has dominant topic 3\n",
      "Document 116 has dominant topic 1\n",
      "Document 117 has dominant topic 0\n",
      "Document 118 has dominant topic 1\n",
      "Document 119 has dominant topic 4\n",
      "Document 120 has dominant topic 3\n",
      "Document 121 has dominant topic 2\n",
      "Document 122 has dominant topic 2\n",
      "Document 123 has dominant topic 3\n",
      "Document 124 has dominant topic 2\n",
      "Document 125 has dominant topic 2\n",
      "Document 126 has dominant topic 1\n",
      "Document 127 has dominant topic 1\n",
      "Document 128 has dominant topic 4\n",
      "Document 129 has dominant topic 2\n",
      "Document 130 has dominant topic 1\n",
      "Document 131 has dominant topic 0\n",
      "Document 132 has dominant topic 4\n",
      "Document 133 has dominant topic 0\n",
      "Document 134 has dominant topic 0\n",
      "Document 135 has dominant topic 0\n",
      "Document 136 has dominant topic 4\n",
      "Document 137 has dominant topic 2\n",
      "Document 138 has dominant topic 2\n",
      "Document 139 has dominant topic 3\n",
      "Document 140 has dominant topic 3\n",
      "Document 141 has dominant topic 0\n",
      "Document 142 has dominant topic 4\n",
      "Document 143 has dominant topic 4\n",
      "Document 144 has dominant topic 1\n",
      "Document 145 has dominant topic 3\n",
      "Document 146 has dominant topic 4\n",
      "Document 147 has dominant topic 0\n",
      "Document 148 has dominant topic 4\n",
      "Document 149 has dominant topic 4\n",
      "Document 150 has dominant topic 4\n",
      "Document 151 has dominant topic 1\n",
      "Document 152 has dominant topic 1\n",
      "Document 153 has dominant topic 0\n",
      "Document 154 has dominant topic 0\n",
      "Document 155 has dominant topic 0\n",
      "Document 156 has dominant topic 0\n",
      "Document 157 has dominant topic 3\n",
      "Document 158 has dominant topic 3\n",
      "Document 159 has dominant topic 4\n",
      "Document 160 has dominant topic 4\n",
      "Document 161 has dominant topic 3\n",
      "Document 162 has dominant topic 4\n",
      "Document 163 has dominant topic 2\n",
      "Document 164 has dominant topic 4\n",
      "Document 165 has dominant topic 4\n",
      "Document 166 has dominant topic 0\n",
      "Document 167 has dominant topic 0\n",
      "Document 168 has dominant topic 4\n",
      "Document 169 has dominant topic 4\n",
      "Document 170 has dominant topic 2\n",
      "Document 171 has dominant topic 1\n",
      "Document 172 has dominant topic 1\n",
      "Document 173 has dominant topic 3\n",
      "Document 174 has dominant topic 4\n",
      "Document 175 has dominant topic 0\n",
      "Document 176 has dominant topic 4\n",
      "Document 177 has dominant topic 1\n",
      "Document 178 has dominant topic 1\n",
      "Document 179 has dominant topic 1\n",
      "Document 180 has dominant topic 1\n",
      "Document 181 has dominant topic 0\n",
      "Document 182 has dominant topic 3\n",
      "Document 183 has dominant topic 4\n",
      "Document 184 has dominant topic 0\n",
      "Document 185 has dominant topic 3\n",
      "Document 186 has dominant topic 4\n",
      "Document 187 has dominant topic 2\n",
      "Document 188 has dominant topic 0\n",
      "Document 189 has dominant topic 3\n",
      "Document 190 has dominant topic 0\n",
      "Document 191 has dominant topic 3\n",
      "Document 192 has dominant topic 2\n",
      "Document 193 has dominant topic 0\n",
      "Document 194 has dominant topic 2\n",
      "Document 195 has dominant topic 2\n",
      "Document 196 has dominant topic 2\n",
      "Document 197 has dominant topic 3\n",
      "Document 198 has dominant topic 0\n",
      "Document 199 has dominant topic 2\n",
      "Document 200 has dominant topic 3\n",
      "Document 201 has dominant topic 0\n",
      "Document 202 has dominant topic 4\n",
      "Document 203 has dominant topic 4\n",
      "Document 204 has dominant topic 2\n",
      "Document 205 has dominant topic 2\n",
      "Document 206 has dominant topic 4\n",
      "Document 207 has dominant topic 4\n",
      "Document 208 has dominant topic 4\n",
      "Document 209 has dominant topic 1\n",
      "Document 210 has dominant topic 2\n",
      "Document 211 has dominant topic 0\n",
      "Document 212 has dominant topic 2\n",
      "Document 213 has dominant topic 3\n",
      "Document 214 has dominant topic 2\n",
      "Document 215 has dominant topic 0\n",
      "Document 216 has dominant topic 0\n",
      "Document 217 has dominant topic 3\n",
      "Document 218 has dominant topic 4\n",
      "Document 219 has dominant topic 2\n",
      "Document 220 has dominant topic 2\n",
      "Document 221 has dominant topic 2\n",
      "Document 222 has dominant topic 1\n",
      "Document 223 has dominant topic 0\n",
      "Document 224 has dominant topic 2\n",
      "Document 225 has dominant topic 4\n",
      "Document 226 has dominant topic 0\n",
      "Document 227 has dominant topic 3\n",
      "Document 228 has dominant topic 1\n",
      "Document 229 has dominant topic 1\n",
      "Document 230 has dominant topic 1\n",
      "Document 231 has dominant topic 4\n",
      "Document 232 has dominant topic 4\n",
      "Document 233 has dominant topic 4\n",
      "Document 234 has dominant topic 0\n",
      "Document 235 has dominant topic 4\n",
      "Document 236 has dominant topic 0\n",
      "Document 237 has dominant topic 0\n",
      "Document 238 has dominant topic 0\n",
      "Document 239 has dominant topic 0\n",
      "Document 240 has dominant topic 0\n",
      "Document 241 has dominant topic 2\n",
      "Document 242 has dominant topic 2\n",
      "Document 243 has dominant topic 4\n",
      "Document 244 has dominant topic 0\n",
      "Document 245 has dominant topic 0\n",
      "Document 246 has dominant topic 3\n",
      "Document 247 has dominant topic 0\n",
      "Document 248 has dominant topic 0\n",
      "Document 249 has dominant topic 0\n",
      "Document 250 has dominant topic 4\n",
      "Document 251 has dominant topic 4\n",
      "Document 252 has dominant topic 0\n",
      "Document 253 has dominant topic 0\n",
      "Document 254 has dominant topic 3\n",
      "Document 255 has dominant topic 0\n",
      "Document 256 has dominant topic 3\n",
      "Document 257 has dominant topic 2\n",
      "Document 258 has dominant topic 2\n",
      "Document 259 has dominant topic 0\n",
      "Document 260 has dominant topic 2\n",
      "Document 261 has dominant topic 2\n",
      "Document 262 has dominant topic 3\n",
      "Document 263 has dominant topic 3\n",
      "Document 264 has dominant topic 4\n",
      "Document 265 has dominant topic 2\n",
      "Document 266 has dominant topic 0\n",
      "Document 267 has dominant topic 2\n",
      "Document 268 has dominant topic 1\n",
      "Document 269 has dominant topic 3\n",
      "Document 270 has dominant topic 3\n",
      "Document 271 has dominant topic 0\n",
      "Document 272 has dominant topic 3\n",
      "Document 273 has dominant topic 3\n",
      "Document 274 has dominant topic 4\n",
      "Document 275 has dominant topic 2\n",
      "Document 276 has dominant topic 1\n",
      "Document 277 has dominant topic 1\n",
      "Document 278 has dominant topic 3\n",
      "Document 279 has dominant topic 0\n",
      "Document 280 has dominant topic 4\n",
      "Document 281 has dominant topic 4\n",
      "Document 282 has dominant topic 0\n",
      "Document 283 has dominant topic 1\n",
      "Document 284 has dominant topic 0\n",
      "Document 285 has dominant topic 0\n",
      "Document 286 has dominant topic 4\n",
      "Document 287 has dominant topic 4\n",
      "Document 288 has dominant topic 3\n",
      "Document 289 has dominant topic 2\n",
      "Document 290 has dominant topic 4\n",
      "Document 291 has dominant topic 4\n",
      "Document 292 has dominant topic 0\n",
      "Document 293 has dominant topic 2\n",
      "Document 294 has dominant topic 1\n",
      "Document 295 has dominant topic 0\n",
      "Document 296 has dominant topic 0\n",
      "Document 297 has dominant topic 1\n",
      "Document 298 has dominant topic 2\n",
      "Document 299 has dominant topic 1\n",
      "Document 300 has dominant topic 0\n",
      "Document 301 has dominant topic 1\n",
      "Document 302 has dominant topic 0\n",
      "Document 303 has dominant topic 3\n",
      "Document 304 has dominant topic 1\n",
      "Document 305 has dominant topic 2\n",
      "Document 306 has dominant topic 3\n",
      "Document 307 has dominant topic 2\n",
      "Document 308 has dominant topic 4\n",
      "Document 309 has dominant topic 2\n",
      "Document 310 has dominant topic 1\n",
      "Document 311 has dominant topic 0\n",
      "Document 312 has dominant topic 0\n",
      "Document 313 has dominant topic 4\n",
      "Document 314 has dominant topic 2\n",
      "Document 315 has dominant topic 1\n",
      "Document 316 has dominant topic 1\n",
      "Document 317 has dominant topic 1\n",
      "Document 318 has dominant topic 2\n",
      "Document 319 has dominant topic 4\n",
      "Document 320 has dominant topic 2\n",
      "Document 321 has dominant topic 1\n",
      "Document 322 has dominant topic 1\n",
      "Document 323 has dominant topic 1\n",
      "Document 324 has dominant topic 2\n",
      "Document 325 has dominant topic 4\n",
      "Document 326 has dominant topic 4\n",
      "Document 327 has dominant topic 4\n",
      "Document 328 has dominant topic 2\n",
      "Document 329 has dominant topic 3\n",
      "Document 330 has dominant topic 4\n",
      "Document 331 has dominant topic 0\n",
      "Document 332 has dominant topic 2\n",
      "Document 333 has dominant topic 0\n",
      "Document 334 has dominant topic 0\n",
      "Document 335 has dominant topic 1\n",
      "Document 336 has dominant topic 1\n",
      "Document 337 has dominant topic 1\n",
      "Document 338 has dominant topic 1\n",
      "Document 339 has dominant topic 2\n",
      "Document 340 has dominant topic 2\n",
      "Document 341 has dominant topic 1\n",
      "Document 342 has dominant topic 1\n",
      "Document 343 has dominant topic 3\n",
      "Document 344 has dominant topic 3\n",
      "Document 345 has dominant topic 1\n",
      "Document 346 has dominant topic 4\n",
      "Document 347 has dominant topic 1\n",
      "Document 348 has dominant topic 1\n",
      "Document 349 has dominant topic 4\n",
      "Document 350 has dominant topic 1\n",
      "Document 351 has dominant topic 4\n",
      "Document 352 has dominant topic 2\n",
      "Document 353 has dominant topic 3\n",
      "Document 354 has dominant topic 3\n",
      "Document 355 has dominant topic 2\n",
      "Document 356 has dominant topic 0\n",
      "Document 357 has dominant topic 2\n",
      "Document 358 has dominant topic 2\n",
      "Document 359 has dominant topic 3\n",
      "Document 360 has dominant topic 0\n",
      "Document 361 has dominant topic 4\n",
      "Document 362 has dominant topic 2\n",
      "Document 363 has dominant topic 1\n",
      "Document 364 has dominant topic 2\n",
      "Document 365 has dominant topic 4\n",
      "Document 366 has dominant topic 4\n",
      "Document 367 has dominant topic 4\n",
      "Document 368 has dominant topic 2\n",
      "Document 369 has dominant topic 2\n",
      "Document 370 has dominant topic 0\n",
      "Document 371 has dominant topic 0\n",
      "Document 372 has dominant topic 3\n",
      "Document 373 has dominant topic 3\n",
      "Document 374 has dominant topic 2\n",
      "Document 375 has dominant topic 3\n",
      "Document 376 has dominant topic 0\n",
      "Document 377 has dominant topic 0\n",
      "Document 378 has dominant topic 2\n",
      "Document 379 has dominant topic 0\n",
      "Document 380 has dominant topic 0\n",
      "Document 381 has dominant topic 3\n",
      "Document 382 has dominant topic 0\n",
      "Document 383 has dominant topic 4\n",
      "Document 384 has dominant topic 4\n",
      "Document 385 has dominant topic 3\n",
      "Document 386 has dominant topic 0\n",
      "Document 387 has dominant topic 0\n",
      "Document 388 has dominant topic 0\n",
      "Document 389 has dominant topic 0\n",
      "Document 390 has dominant topic 2\n",
      "Document 391 has dominant topic 2\n",
      "Document 392 has dominant topic 3\n",
      "Document 393 has dominant topic 3\n",
      "Document 394 has dominant topic 3\n",
      "Document 395 has dominant topic 4\n",
      "Document 396 has dominant topic 4\n",
      "Document 397 has dominant topic 3\n",
      "Document 398 has dominant topic 0\n",
      "Document 399 has dominant topic 4\n",
      "Document 400 has dominant topic 4\n",
      "Document 401 has dominant topic 2\n",
      "Document 402 has dominant topic 0\n",
      "Document 403 has dominant topic 3\n",
      "Document 404 has dominant topic 2\n",
      "Document 405 has dominant topic 2\n",
      "Document 406 has dominant topic 2\n",
      "Document 407 has dominant topic 0\n",
      "Document 408 has dominant topic 4\n",
      "Document 409 has dominant topic 3\n",
      "Document 410 has dominant topic 4\n",
      "Document 411 has dominant topic 4\n",
      "Document 412 has dominant topic 4\n",
      "Document 413 has dominant topic 0\n",
      "Document 414 has dominant topic 2\n",
      "Document 415 has dominant topic 0\n",
      "Document 416 has dominant topic 0\n",
      "Document 417 has dominant topic 1\n",
      "Document 418 has dominant topic 0\n",
      "Document 419 has dominant topic 0\n",
      "Document 420 has dominant topic 0\n",
      "Document 421 has dominant topic 0\n",
      "Document 422 has dominant topic 0\n",
      "Document 423 has dominant topic 0\n",
      "Document 424 has dominant topic 0\n",
      "Document 425 has dominant topic 0\n",
      "Document 426 has dominant topic 1\n",
      "Document 427 has dominant topic 3\n",
      "Document 428 has dominant topic 0\n",
      "Document 429 has dominant topic 1\n",
      "Document 430 has dominant topic 0\n",
      "Document 431 has dominant topic 0\n",
      "Document 432 has dominant topic 0\n",
      "Document 433 has dominant topic 0\n",
      "Document 434 has dominant topic 2\n",
      "Document 435 has dominant topic 2\n",
      "Document 436 has dominant topic 2\n",
      "Document 437 has dominant topic 0\n",
      "Document 438 has dominant topic 0\n",
      "Document 439 has dominant topic 0\n",
      "Document 440 has dominant topic 2\n",
      "Document 441 has dominant topic 2\n",
      "Document 442 has dominant topic 0\n",
      "Document 443 has dominant topic 3\n",
      "Document 444 has dominant topic 2\n",
      "Document 445 has dominant topic 0\n",
      "Document 446 has dominant topic 2\n",
      "Document 447 has dominant topic 0\n",
      "Document 448 has dominant topic 1\n",
      "Document 449 has dominant topic 3\n",
      "Document 450 has dominant topic 2\n",
      "Document 451 has dominant topic 4\n",
      "Document 452 has dominant topic 0\n",
      "Document 453 has dominant topic 1\n",
      "Document 454 has dominant topic 2\n",
      "Document 455 has dominant topic 2\n",
      "Document 456 has dominant topic 0\n",
      "Document 457 has dominant topic 2\n",
      "Document 458 has dominant topic 3\n",
      "Document 459 has dominant topic 0\n",
      "Document 460 has dominant topic 2\n",
      "Document 461 has dominant topic 1\n",
      "Document 462 has dominant topic 0\n",
      "Document 463 has dominant topic 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dominant_topics = np.argmax(W, axis=1)  # Get the index of the max value in each row\n",
    "\n",
    "# Print the dominant topic for each document\n",
    "for i, topic_num in enumerate(dominant_topics):\n",
    "    print(f\"Document {i} has dominant topic {topic_num}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 has 134 documents.\n",
      "Topic 1 has 62 documents.\n",
      "Topic 2 has 97 documents.\n",
      "Topic 3 has 74 documents.\n",
      "Topic 4 has 97 documents.\n"
     ]
    }
   ],
   "source": [
    "topic_counts = np.bincount(dominant_topics)\n",
    "for topic_index, count in enumerate(topic_counts):\n",
    "    print(f\"Topic {topic_index} has {count} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 - Observational Comedy: life, dog, wife, eat, jesus, home, car, whole, head, work\n",
      "Topic 1 - British Comedy: mum, bloke, wee, london, lovely, accent, fat, shite, brilliant, glasgow\n",
      "Topic 2 - American | Political Comedy: president, america, donald, country, obama, money, police, women, american, folks\n",
      "Topic 3 - Sexual  LGBTQ Comedy: women, men, woman, girl, sex, gay, baby, life, bad, pregnant\n",
      "Topic 4 - Family Life Comedy: mom, dad, son, kid, life, parents, friends, baby, school, bro\n"
     ]
    }
   ],
   "source": [
    "topics = {\n",
    "    0: \"Observational Comedy\",\n",
    "    1: \"British Comedy\",\n",
    "    2: \"American | Political Comedy\",\n",
    "    3: \"Sexual  LGBTQ Comedy\",\n",
    "    4: \"Family Life Comedy\"\n",
    "}\n",
    "\n",
    "\n",
    "topic_keywords = {\n",
    "    0: [\"life\", \"dog\", \"wife\", \"eat\", \"jesus\", \"home\", \"car\", \"whole\", \"head\", \"work\"],\n",
    "    1: [\"mum\", \"bloke\", \"wee\", \"london\", \"lovely\", \"accent\", \"fat\", \"shite\", \"brilliant\", \"glasgow\"],\n",
    "    2: [\"president\", \"america\", \"donald\", \"country\", \"obama\", \"money\", \"police\", \"women\", \"american\", \"folks\"],\n",
    "    3: [\"women\", \"men\", \"woman\", \"girl\", \"sex\", \"gay\", \"baby\", \"life\", \"bad\", \"pregnant\"],\n",
    "    4: [\"mom\", \"dad\", \"son\", \"kid\", \"life\", \"parents\", \"friends\", \"baby\", \"school\", \"bro\"]\n",
    "}\n",
    "\n",
    "\n",
    "for topic_num, label in topics.items():\n",
    "    keywords = ', '.join(topic_keywords[topic_num])\n",
    "    print(f\"Topic {topic_num} - {label}: {keywords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 has 134 documents.\n",
      "Topic 1 has 62 documents.\n",
      "Topic 2 has 97 documents.\n",
      "Topic 3 has 74 documents.\n",
      "Topic 4 has 97 documents.\n"
     ]
    }
   ],
   "source": [
    "topic_counts = np.bincount(dominant_topics)\n",
    "for topic_index, count in enumerate(topic_counts):\n",
    "    print(f\"Topic {topic_index} has {count} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''from gensim.models.coherencemodel import CoherenceModel\n",
    "#from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "\n",
    "#original_texts = nmfdf['select_tokens_nmf'].tolist()\n",
    "#dictionary = Dictionary(original_texts)\n",
    "#corpus = [dictionary.doc2bow(text) for text in original_texts]\n",
    "\n",
    "\n",
    "#top_words_per_topic = []\n",
    "#for topic_idx, topic in enumerate(H):\n",
    "    #top_features_ind = topic.argsort()[-10:][::-1]\n",
    "    #top_features = [feature_names[i] for i in top_features_ind]\n",
    "    #top_words_per_topic.append(top_features)\n",
    "\n",
    "\n",
    "#coherence_model = CoherenceModel(topics=top_words_per_topic, texts=original_texts, dictionary=dictionary, coherence='c_v')\n",
    "#coherence_score = coherence_model.get_coherence()\n",
    "#print(f'Coherence Score: {coherence_score}')'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
